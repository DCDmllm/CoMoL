{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/miniconda3/envs/moa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import get_formatted_datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test'],\n",
      "        num_rows: 164\n",
      "    })\n",
      "})\n",
      "Example: {'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'entry_point': 'has_close_elements', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 164/164 [00:00<00:00, 17324.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted datasets: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['task_id', 'prompt', 'entry_point', 'canonical_solution', 'test', 'instruction', 'data_name', 'text'],\n",
      "        num_rows: 164\n",
      "    })\n",
      "})\n",
      "Formatted example: {'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'entry_point': 'has_close_elements', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", 'instruction': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'data_name': 'humaneval', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nfrom typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n\\n\\n### Response:'}\n",
      "Text example:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path =\"/data/workspace/projects/moe/datasets/eval_code\"\n",
    "data_name = \"humaneval\"\n",
    "data_path_1 = os.path.join(data_path, data_name)\n",
    "data_name = data_name.lower()\n",
    "# Load and format datasets\n",
    "formatted_datasets = get_formatted_datasets(data_path=data_path_1, prompt_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_file = '/data/workspace/projects/moe/AdaMoLE/outputs/llama-3-1-8b-instruct-lora-codealpaca20k_3epoch/predictions/humaneval_responses.jsonl'\n",
    "mocorelora_corerouter_file = '/data/workspace/projects/moe/AdaMoLE/outputs/llama-3-1-8b-instruct-mocorelora-exp8-corerouter-codealpaca20k/predictions/humaneval_responses.jsonl'\n",
    "mbpp_lora_file = '/data/workspace/projects/moe/AdaMoLE/outputs/llama-3-1-8b-instruct-lora-codealpaca20k_3epoch/predictions/mbpp_responses.jsonl'\n",
    "mbpp_mocorelora_corerouter_file = '/data/workspace/projects/moe/AdaMoLE/outputs/llama-3-1-8b-instruct-mocorelora-exp8-corerouter-codealpaca20k/predictions/mbpp_responses.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_utils.env import get_feedback, is_correct\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(predict_file, 'r') as f:\n",
    "with open(mocorelora_corerouter_file, 'r') as f:\n",
    "# with open(mbpp_lora_file, 'r') as f:\n",
    "# with open(mbpp_mocorelora_corerouter_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "predicts = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredicts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m499\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predicts[499]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  eval code from zhenghaoyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6402439024390244\n"
     ]
    }
   ],
   "source": [
    "# humaneval\n",
    "correct_count = 0\n",
    "for predict in predicts:\n",
    "    code = predict['response']\n",
    "    test = predict['test']\n",
    "    correct = is_correct(code, test)\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "\n",
    "print(correct_count/len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mbpp\n",
    "correct_count = 0\n",
    "for i,predict in enumerate(predicts):\n",
    "    code = predict['response']\n",
    "    test = predict['test_list']\n",
    "    print(i)\n",
    "    if i in (125,126):\n",
    "        continue\n",
    "    # is_passing, feedback = get_feedback(code, test)\n",
    "    # if is_passing:\n",
    "    #     correct_count += 1\n",
    "    # else:\n",
    "    #     pass\n",
    "        # print(feedback)\n",
    "    \n",
    "    test = '\\n'.join(test)\n",
    "    correct = is_correct(code, test)\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "\n",
    "print(correct_count/len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human eval 代码评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = [json.loads(line) for line in lines]\n",
    "samples = []\n",
    "for x in predicts:\n",
    "    if type(x['response']) == list:\n",
    "        for response in x['response']:\n",
    "            sample = {}\n",
    "            sample['task_id'] = x['task_id']\n",
    "            sample['completion'] = response\n",
    "            samples.append(sample)\n",
    "    else:\n",
    "        sample = {}\n",
    "        sample['task_id'] = x['task_id']\n",
    "        sample['completion'] = x['response']\n",
    "        samples.append(sample)\n",
    "write_jsonl('sample.jsonl', samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "1640it [00:00, 95986.19it/s]\n",
      "Running test suites...\n",
      "100%|██████████████████████████████████████| 1640/1640 [00:14<00:00, 112.52it/s]\n",
      "Writing results to sample.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████| 1640/1640 [00:00<00:00, 142562.87it/s]\n",
      "{'pass@1': np.float64(0.29329268292682925), 'pass@5': np.float64(0.5536924119241191), 'pass@10': np.float64(0.6524390243902439)}\n"
     ]
    }
   ],
   "source": [
    "# ! evaluate_functional_correctness sample.jsonl \n",
    "# ! evaluate_functional_correctness sample.jsonl --problem_file=/data/workspace/projects/moe/datasets/mbpp_forhumaneval.jsonl\n",
    "! evaluate_functional_correctness sample.jsonl --k='\"1,5,10\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.evaluation import evaluate_functional_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [00:00, 27432.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:20<00:00, 79.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to sample.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:00<00:00, 141974.38it/s]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_functional_correctness('sample.jsonl',k=[1,5,10])\n",
    "result\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get mbpp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "\n",
    "data_source = \"Muennighoff/mbpp\"\n",
    "\n",
    "dataset = datasets.load_dataset(data_source)\n",
    "\n",
    "test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = test.filter(lambda x: 11 <= x[\"task_id\"] <= 510)\n",
    "filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 115.03ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "287032"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.to_json(\"/data/workspace/projects/moe/datasets/eval_code/mbpp/test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "    num_rows: 974\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert mbpp to human eval formart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/workspace/projects/moe/datasets/eval_code/mbpp/test.jsonl', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 11,\n",
       " 'text': 'Write a python function to remove first and last occurrence of a given character from the string.',\n",
       " 'code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ',\n",
       " 'test_list': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"',\n",
       "  'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"',\n",
       "  'assert remove_Occ(\"PHP\",\"P\") == \"H\"'],\n",
       " 'test_setup_code': '',\n",
       " 'challenge_test_list': ['assert remove_Occ(\"hellolloll\",\"l\") == \"helollol\"',\n",
       "  'assert remove_Occ(\"\",\"l\") == \"\"']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "new_data = []\n",
    "for x in data:\n",
    "    new_x = {}\n",
    "    new_x['task_id'] = x['task_id']\n",
    "    new_x['prompt'] = ''\n",
    "    match = re.search(r'assert\\s+([a-zA-Z_]\\w*)\\s*\\(', x['test_list'][0])\n",
    "    func_name = match.group(1)\n",
    "    new_x['entry_point'] = func_name\n",
    "    new_x['test'] = 'def check(candidate):\\n\\n    ' + '\\n    '.join(x['test_list']).replace(func_name, 'candidate')\n",
    "    new_data.append(new_x)\n",
    "with open('/data/workspace/projects/moe/datasets/eval_code/mbpp/test_forhumaneval.jsonl', 'a') as f:\n",
    "    for x in new_data:\n",
    "        f.write(json.dumps(x) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_string\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"assert extract_string(['Python', 'list',)\"\n",
    "match = re.search(r'assert\\s+([a-zA-Z_]\\w*)\\s*\\(', s)\n",
    "if match:\n",
    "    func_name = match.group(1)\n",
    "    print(func_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
